# ğŸ›¡ï¸ Cybersecurity Incident Data Analysis  
_End-to-end ETL & Analytics Pipeline (Power Query + Power BI)_

This project demonstrates a complete data workflow that an **IT Analyst / Cybersecurity Analyst** would typically manage:

- Data cleaning & standardization  
- ETL pipeline creation using Power Query  
- Incident normalization according to SOC/ITSM logic  
- BI-ready dataset generation  
- Dashboard planning & KPI design  
- Documentation & analytics methodology  

_All data is 100% synthetic and created for learning and portfolio purposes._

---

## ğŸ“ Repository Structure

```
Cybersecurity-Incident-Data-Analysis/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_cyber_incidents.csv
â”‚   â””â”€â”€ clean_cyber_incidents.csv
â”‚
â”œâ”€â”€ powerquery/
â”‚   â””â”€â”€ PowerQuery_Cleaning_Steps.m
â”‚
â”œâ”€â”€ dashboard/                 â† (To Do)
â”‚   â”œâ”€â”€ cybersecurity_dashboard.pbix
â”‚   â””â”€â”€ screenshots/
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§¾ Dataset Description

The dataset simulates 50â€“300 cybersecurity incidents with fields commonly used in:

- SOC monitoring  
- Incident response workflows  
- ITSM ticketing  
- Internal audit / GRC processes  

Typical fields include:

- `Incident_ID` â€“ Unique identifier  
- `Date_Incident` â€“ Datetime of detection/logging  
- `Region` â€“ e.g. LU, BE, FR, DE  
- `Department_Affected` â€“ IT, Finance, HR, Operationsâ€¦  
- `Asset_Type` â€“ Server, Workstation, Network Device, Cloud Serviceâ€¦  
- `Incident_Type` â€“ Phishing, Malware, Ransomware, Unauthorized Access, etc.  
- `Severity` â€“ Low / Medium / High / Critical  
- `Priority` â€“ P1â€“P4 (inconsistent in the raw data)  
- `Detection_Source` â€“ SIEM, IDS/IPS, EDR, Email Gateway, User Reportâ€¦  
- `Status` â€“ Contained, Resolved, In Progress, False Positiveâ€¦  
- `Time_to_Detect_min`, `Time_to_Contain_min`, `Time_to_Resolve_min`  
- `Downtime_min` â€“ Business downtime in minutes  
- `Estimated_Cost_EUR` â€“ Estimated financial impact  
- `Business_Impact` â€“ None / Minor / Moderate / Major  
- `Root_Cause` â€“ Misconfiguration, Vulnerability, Social Engineering, etc.  
- `Data_Sensitivity` â€“ Public / Internal / Confidential / Restricted  
- `SLA_Breached` â€“ Yes / No  
- `Analyst` â€“ Assigned SOC analyst  

---

## ğŸ§¹ Power Query ETL Pipeline

The full ETL logic is implemented in:

- `powerquery/PowerQuery_Cleaning_Steps.m`

It transforms:

`data/raw_cyber_incidents.csv` âœ `data/clean_cyber_incidents.csv`

### ğŸ”§ Main Transformations  
_(directly aligned with the M-script)_

- **Trim & normalize text fields**  
  - Remove leading/trailing whitespace from Region, Department, Asset_Type, Incident_Type, Severity, Priority, Detection_Source, Status, Business_Impact, Root_Cause, Data_Sensitivity, SLA_Breached, Analyst.
- **Standardize incident types**  
  - `Phising`, `Phshing` â†’ `Phishing`  
  - `Maleware`, `Mal-ware` â†’ `Malware`  
  - `Ransomeware`, `Ransom-ware` â†’ `Ransomware`  
  - `DDOS`, `Ddos` â†’ `DDoS`  
  - Other small spelling variants are normalized as well.
- **Normalize priority levels (P1â€“P4)**  
  - Map free-text formats to a clean scale:  
    - Contains â€œp1â€ or â€œcriticalâ€ â†’ `P1 - Critical`  
    - Contains â€œp2â€ or â€œhighâ€ â†’ `P2 - High`  
    - Contains â€œp3â€ or â€œmediumâ€ â†’ `P3 - Medium`  
    - Contains â€œp4â€ or â€œlowâ€ â†’ `P4 - Low`
- **Enforce consistency from Severity**  
  - If `Severity = "Critical"` â†’ `Priority = "P1 - Critical"`  
  - If `Severity = "High"` â†’ `P2 - High`  
  - If `Severity = "Medium"` â†’ `P3 - Medium`  
  - Else â†’ `P4 - Low`
- **Convert durations to numeric values**  
  - Remove `" min"` suffix from `Time_to_*_min` and `Downtime_min`  
  - Convert to numeric (handling errors and invalid formats)  
  - Replace negative values with `null`
- **Extract numeric cost**  
  - Keep only digits in `Estimated_Cost_EUR`  
  - Cast to integer
- **Parse dates**  
  - Convert `Date_Incident` to `datetime` (with a `try ... otherwise null` pattern)
- **Handle missing values**  
  - Replace `null` in `Detection_Source`, `Root_Cause`, `Data_Sensitivity`, `SLA_Breached` with `"Unknown"`
- **Sort & deduplicate**  
  - Sort by `Incident_ID` and `Date_Incident` (descending)  
  - Deduplicate on `Incident_ID` and keep the latest incident record
- **Derived fields**  
  - `Year` â€“ Year from `Date_Incident`  
  - `Month` â€“ Month name from `Date_Incident`  
  - `Resolve_Hours` â€“ `Time_to_Resolve_min / 60` (rounded)  
  - `Is_Critical` â€“ 1 if `Severity = "Critical"`, else 0

---

## ğŸ“˜ Data Dictionary (Key Fields)

| Field                  | Description                                        |
|------------------------|----------------------------------------------------|
| `Incident_ID`          | Unique incident identifier                         |
| `Date_Incident`        | Datetime of incident detection/logging             |
| `Region`               | Region or country                                  |
| `Department_Affected`  | Impacted department                                |
| `Asset_Type`           | Type of impacted asset                             |
| `Incident_Type`        | Normalized incident category                       |
| `Severity`             | Low / Medium / High / Critical                     |
| `Priority`             | Standardized P1â€“P4 priority                        |
| `Time_to_Detect_min`   | Minutes to detect                                  |
| `Time_to_Contain_min`  | Minutes to contain                                 |
| `Time_to_Resolve_min`  | Minutes to fully resolve                           |
| `Resolve_Hours`        | Derived resolution time in hours                   |
| `Downtime_min`         | Total business downtime in minutes                 |
| `Estimated_Cost_EUR`   | Estimated cost in EUR (integer)                    |
| `Business_Impact`      | Business impact level                              |
| `Root_Cause`           | Underlying cause of incident                       |
| `Data_Sensitivity`     | Data classification level                          |
| `SLA_Breached`         | SLA breach flag (Yes/No/Unknown)                   |
| `Is_Critical`          | 1 if Severity = Critical, else 0                   |
| `Year`                 | Year extracted from `Date_Incident`                |
| `Month`                | Month name extracted from `Date_Incident`          |

---

## ğŸ§© Architecture Diagram (Pipeline)

Below is a high-level view of the data pipeline from raw CSV to BI-ready dataset and dashboard.

```mermaid
flowchart LR
    A[Raw CSV<br/>data/raw_cyber_incidents.csv] --> B[Power Query ETL<br/>(PowerQuery_Cleaning_Steps.m)]
    B --> C[Cleaned CSV<br/>data/clean_cyber_incidents.csv]
    C --> D[Power BI Model<br/>(Fact table)]
    D --> E[Dashboards<br/>Executive / Operational / Risk]
```

**Stages:**

1. **Source** â€” Synthetic incidents stored as a flat CSV.  
2. **ETL (Power Query)** â€” All cleaning, normalization and enrichment applied.  
3. **Cleaned Layer** â€” Single fact-like table ready for BI tools.  
4. **Analytics Layer (Power BI)** â€” Visual dashboards (KPIs, trends, cost, risk).  

---

## ğŸ“Š Power BI Dashboard â€” Planned (To Do)

### ğŸŸ¦ Page 1 â€” Executive Overview
- Total number of incidents  
- % Critical incidents  
- SLA breach rate  
- Total estimated cost (EUR)  
- Incident trend by month & year  

### ğŸŸ¨ Page 2 â€” Operational Insights
- Incidents by type and severity  
- MTTR (Mean Time To Resolve) by severity  
- MTTD (Mean Time To Detect) / MTTC (Mean Time To Contain)  
- Detection source breakdown (tool vs user-reported)  
- Top 10 costliest incidents  

### ğŸŸ¥ Page 3 â€” Risk & Governance
- Business impact distribution by department/region  
- Downtime vs severity / incident type  
- SLA breaches vs priority / severity  
- Critical incident clustering / patterns  

**Dashboard files (To Do):**

- `dashboard/cybersecurity_dashboard.pbix`  
- `dashboard/screenshots/` (key visual exports)

---

## ğŸ’¡ Insights & Recommendations (Conceptual)

These insights are examples of what the cleaned dataset and future Power BI dashboard are designed to surface. They illustrate how this pipeline can support IT / security decision-making:

### Example Insights (based on typical patterns)

- **Critical incidents are often linked to specific incident types**  
  e.g. Ransomware, Data Exfiltration or Unauthorized Access drive a disproportionate share of `Critical` severity cases.

- **User-reported incidents tend to have higher MTTD**  
  Incidents detected via â€œUser Reportâ€ usually take longer to detect compared to SIEM/EDR, which can increase SLA breach risk.

- **A small number of incidents may drive most of the cost**  
  Top 5â€“10 incidents (by `Estimated_Cost_EUR`) often represent a significant share of the total impact (Pareto-style pattern).

- **Some departments or regions are repeatedly over-represented**  
  Higher incident counts or critical incidents might cluster in specific departments (e.g. Operations / Finance) or regions.

- **Configuration and patching issues can be recurring root causes**  
  â€œMisconfigurationâ€ or â€œUnpatched Vulnerabilityâ€ may appear frequently in `Root_Cause`, especially for high-cost incidents.

### Recommendations (conceptual)

- **Improve early detection**  
  - Strengthen SIEM / EDR rules for top incident types (e.g. phishing, malware).  
  - Reduce reliance on user-only reports for critical incident types.

- **Focus on high-impact segments**  
  - Prioritize hardening and awareness in departments/regions with repeated high-severity incidents.  
  - Define targeted training for users in high-risk areas.

- **Reduce SLA breaches**  
  - Monitor MTTD and MTTR for P1/P2 to ensure they stay within defined thresholds.  
  - Implement playbooks to accelerate containment and resolution for critical categories.

- **Use this dataset as a template**  
  - Extend the model with real logs and ITSM exports (e.g. ServiceNow, Jira, etc.).  
  - Add more tables (assets, vulnerabilities, controls) for deeper risk analysis.

---

## ğŸ—‚ï¸ Kanban-Style To Do / Done

| Status | Item | Details |
|--------|------|---------|
| âœ… Done | Synthetic raw dataset | Generated `data/raw_cyber_incidents.csv` |
| âœ… Done | ETL via Power Query | Implemented in `powerquery/PowerQuery_Cleaning_Steps.m` |
| âœ… Done | Cleaned dataset | Exported `data/clean_cyber_incidents.csv` |
| âœ… Done | Repo documentation | This README (ETL, structure, roadmap) |
| â³ To Do | Build Power BI dashboard | Create `dashboard/cybersecurity_dashboard.pbix` |
| â³ To Do | Add screenshots | Save key visuals in `dashboard/screenshots/` |
| â³ To Do | Insights & recommendations (final) | Refine based on actual dashboard metrics |
| â³ To Do | Optional Python validation | Add a notebook to validate ETL with pandas |

---

## ğŸ”® Roadmap

- [x] Generate synthetic raw dataset  
- [x] Build Power Query ETL pipeline  
- [x] Export cleaned dataset  
- [x] Document ETL, data dictionary and architecture  
- [ ] Build Power BI dashboard (3 pages)  
- [ ] Add screenshots of key visuals  
- [ ] Write final â€œInsights & Recommendationsâ€ based on actual visuals  
- [ ] (Optional) Add Python notebook to validate metrics and spot-check data  

---

## ğŸ“… Update Log

**2025-11-16**  
- Added cleaned dataset (`data/clean_cyber_incidents.csv`)  
- Normalized data folder structure  
- Documented ETL pipeline, data dictionary & architecture  
- Added dashboard plan, insights section & Kanban-style roadmap  

**2025-10-25**  
- Initial commit (`raw_cyber_incidents.csv` + Power Query script)

---

## ğŸ“„ License  

MIT License  

---

ğŸŸ£ _All data in this repository is synthetic and intended solely for training, portfolio building, and interview preparation (no real incidents or sensitive information)._



